{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import fitz\n",
    "import pickle\n",
    "import requests\n",
    "import pandas as pd\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read `DataSet.xlsx`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = \"DataSet.xlsx\"\n",
    "train_data = pd.read_excel(file_path, sheet_name=\"train_data\")\n",
    "test_data = pd.read_excel(file_path, sheet_name=\"test_data\")\n",
    "train_data, test_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Download PDF files to `dataset/` directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to download a PDF and map its path to the label\n",
    "def download_pdf_with_label(url, label, folder, mapping):\n",
    "    \"\"\"Downloads a PDF from a URL, saves it, and maps its path to the label.\"\"\"\n",
    "    file_name = url.split(\"/\")[-1]\n",
    "    save_path = os.path.join(folder, file_name)\n",
    "    try:\n",
    "        response = requests.get(url, stream=True, timeout=10)\n",
    "        response.raise_for_status()  # Raise error for bad status codes\n",
    "        with open(save_path, \"wb\") as f:\n",
    "            f.write(response.content)\n",
    "        print(f\"Downloaded: {save_path}\")\n",
    "        mapping.append({\"file_path\": save_path, \"label\": label})\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Error downloading {url}: {e}\")\n",
    "\n",
    "\n",
    "# Function to process a dataset\n",
    "def process_dataset(data, folder, max_threads=100):\n",
    "    \"\"\"\n",
    "    Downloads files for a dataset, saves them in the specified folder,\n",
    "    and returns a mapping of file paths to labels, then saves it as a CSV or Excel.\n",
    "    \"\"\"\n",
    "    # Initialize an empty list to store the mappings\n",
    "    mapping = []  # To store file path and label mappings\n",
    "    urls_labels = data[[\"datasheet_link\", \"target_col\"]].to_dict(\n",
    "        orient=\"records\"\n",
    "    )  # Convert to a list of dicts\n",
    "\n",
    "    # Download the PDFs and populate the mapping\n",
    "    with ThreadPoolExecutor(max_threads) as executor:\n",
    "        # Pass each URL and label to the downloader function\n",
    "        executor.map(\n",
    "            lambda x: download_pdf_with_label(\n",
    "                x[\"datasheet_link\"], x[\"target_col\"], folder, mapping\n",
    "            ),\n",
    "            urls_labels,\n",
    "        )\n",
    "\n",
    "    # Convert the mapping list into a Pandas DataFrame\n",
    "    mapping_df = pd.DataFrame(mapping, columns=[\"file_path\", \"label\"])\n",
    "\n",
    "    return mapping_df\n",
    "\n",
    "\n",
    "# Directories to save PDFs\n",
    "os.makedirs(\"dataset/train\", exist_ok=True)\n",
    "os.makedirs(\"dataset/test\", exist_ok=True)\n",
    "\n",
    "# Download train datasets and store its mappings\n",
    "train_df = process_dataset(train_data, \"dataset/train\")\n",
    "train_df.to_csv(\"dataset/train.csv\", index=False)\n",
    "print(\"Mapping saved as dataset/train.csv\")\n",
    "\n",
    "# Download test datasets and store its mappings\n",
    "test_df = process_dataset(test_data, \"dataset/test\")\n",
    "test_df.to_csv(\"dataset/test.csv\", index=False)\n",
    "print(\"Mapping saved as dataset/test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extracting and preprocessing data from PDF files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract text from a PDF\n",
    "def extract_text_from_pdf(file_path):\n",
    "    \"\"\"Extracts text from a PDF file.\"\"\"\n",
    "    print(\"Reading content of:\", file_path)\n",
    "    try:\n",
    "        with fitz.open(file_path) as pdf:\n",
    "            text = \"\"\n",
    "            for page in pdf:\n",
    "                text += page.get_text()\n",
    "        return text\n",
    "    except Exception as e:\n",
    "        print(f\"Error extracting text from {file_path}: {e}\")\n",
    "        return \"\"\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"Basic text preprocessing.\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"[^\\w\\s]\", \" \", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\d+\", \" \", text)  # Remove numbers\n",
    "    tokens = text.split()\n",
    "    tokens = [t for t in tokens if t not in ENGLISH_STOP_WORDS]  # Remove stopwords\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "\n",
    "# Read the train and test PDFs\n",
    "train_df = pd.read_csv(\"dataset/train.csv\")\n",
    "test_df = pd.read_csv(\"dataset/test.csv\")\n",
    "\n",
    "# Apply text extraction\n",
    "train_df[\"text\"] = train_df[\"file_path\"].apply(extract_text_from_pdf)\n",
    "test_df[\"text\"] = test_df[\"file_path\"].apply(extract_text_from_pdf)\n",
    "\n",
    "# Preprocess the text\n",
    "train_df[\"clean_text\"] = train_df[\"text\"].apply(preprocess_text)\n",
    "test_df[\"clean_text\"] = test_df[\"text\"].apply(preprocess_text)\n",
    "\n",
    "# Saving train and test files after cleaning and preprocessing\n",
    "train_df.to_pickle(\"dataset/train_cleaned.pkl\")\n",
    "test_df.to_pickle(\"dataset/test_cleaned.pkl\")\n",
    "print(\"Saved cleaned train and test files!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the train and test PDFs\n",
    "train_cleaned = pd.read_pickle(\"dataset/train_cleaned.pkl\")\n",
    "test_cleaned = pd.read_pickle(\"dataset/test_cleaned.pkl\")\n",
    "\n",
    "# Convert text to features using TF-IDF\n",
    "vectorizer = TfidfVectorizer(max_features=1000)\n",
    "X_train = vectorizer.fit_transform(train_cleaned[\"clean_text\"])\n",
    "X_test = vectorizer.transform(test_cleaned[\"clean_text\"])\n",
    "\n",
    "# Get the labels\n",
    "y_train = train_cleaned[\"label\"]\n",
    "y_test = test_cleaned[\"label\"]\n",
    "\n",
    "# Train a Random Forest classifier\n",
    "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions and evaluate\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving Trained Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs(\"models\", exist_ok=True)\n",
    "\n",
    "# Save the trained model\n",
    "with open(\"models/product_classifier_model.pkl\", \"wb\") as model_file:\n",
    "    pickle.dump(model, model_file)\n",
    "\n",
    "# Save the TF-IDF vectorizer\n",
    "with open(\"models/tfidf_vectorizer.pkl\", \"wb\") as vectorizer_file:\n",
    "    pickle.dump(vectorizer, vectorizer_file)\n",
    "\n",
    "# Save the labels\n",
    "with open(\"models/labels.pkl\", \"wb\") as labels_file:\n",
    "    pickle.dump(list(model.classes_), labels_file)\n",
    "\n",
    "print(\"Model, labels and vectorizer saved successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classify PDFs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_pdf(file_path):\n",
    "    \"\"\"Classify a new PDF file.\"\"\"\n",
    "    # Extract and preprocess text from the new file\n",
    "    text = extract_text_from_pdf(file_path)\n",
    "    clean_text = preprocess_text(text)\n",
    "\n",
    "    # Transform the cleaned text into the same feature space as the training data\n",
    "    features = vectorizer.transform([clean_text])\n",
    "\n",
    "    # Predict the class\n",
    "    prediction = model.predict(features)\n",
    "\n",
    "    # Return the predicted label\n",
    "    return prediction[0]\n",
    "\n",
    "\n",
    "# Example usage\n",
    "file_path = \"dataset/test/some_new_pdf.pdf\"\n",
    "predicted_label = classify_pdf(file_path)\n",
    "print(f\"Predicted label: {predicted_label}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
